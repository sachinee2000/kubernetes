 1). namespace object creation

* Shows all namespaces currently in the cluster:
  kubectl get namespaces

* This creates a new namespace named amma
  kubectl create namespace amma

* View namespace details ,Shows labels, annotations, resource quotas, and status of the namespace.
  kubectl describe namespace amma

                                                                                    (first create a custom name space after we set deafult namespace)

* Check current context namespace, show default name space
  kubectl config view --minify | grep namespace:

* Set new default namespace
  kubectl config set-context --current --namespace=amma

* Check current context namespace, show default name space
  kubectl config view --minify | grep namespace:

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                    2). Create secret object
                                                                                                     
* Create secret object command
  kubectl create secret docker-registry my-docker-secret \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=vinaykumars064 \
  --docker-password=vini9535@ \
  --docker-email=vinaykumars064@gmail.com


* üîπ Explanation:

--my-docker-secret ‚Üí Secret name.

--docker-server ‚Üí Registry URL (https://index.docker.io/v1/ for Docker Hub).

--docker-username ‚Üí Your Docker Hub username.

--docker-password ‚Üí Your Docker Hub password / access token.

--docker-email ‚Üí Your email.


*  kubectl get secrets  or  kubectl get svc
*  kubectl describe secret my-docker-secret


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

                                                                                                   
                                                                                              3). pod object creation with secret and container port

Uses a Secret for private Docker registry login

Runs a container with an exposed container port    

* first create one file cold pod.yaml
  touch pod.yaml
  vim pod.yaml
  cat pod.yaml
 

example:::

   apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: my-pod
spec:
  containers:
  - name: my-pod
    image: vinaykumars064/cicd-repo:latest                # private image
    ports:
    - containerPort: 80                            # container port exposed
  imagePullSecrets:
  - name: my-docker-secret                         # secret created earlier



  Steps to Use ::

  *  Apply the Pod
     kubectl apply -f pod.yaml

  *  Verify pod status
     kubectl get pods
     kubectl describe pod my-pod




  ‚úÖ What happens here?

      Kubernetes uses my-docker-secret to log in to Docker Hub (or your registry).

      The Pod pulls private-nginx:latest.

      The container exposes port 80 internally (you can later attach a Service to make it accessible).

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------      

     
                                                                                          4). deployment object creation with secret and container port and replicas



Here‚Äôs a ready-to-use deployment.yaml that creates a Deployment in Kubernetes with:

*Secret for pulling a private Docker image
*Container port exposed
* with 2 replicas


* first create one file cold deployment.yaml
  touch deployment.yaml
  vim deployment.yaml
  cat deployment.yaml
 

example:::



apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
  labels:
    app: my-app
spec:
  replicas: 2                                # number of Pods to run
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: your-docker-user/your-private-image:latest   # replace with your private image
        ports:
        - containerPort: 8080                               # expose port inside container
      imagePullSecrets:
      - name: my-docker-secret                              # secret created earlier




1. Apply the Deployment
kubectl apply -f deployment.yaml

2. Verify
kubectl get deployments
kubectl get pods
kubectl describe deployment my-app-deployment


‚úÖ Explanation

replicas: 2 ‚Üí runs 2 Pods for scaling.

containerPort: 8080 ‚Üí container listens on port 8080.

imagePullSecrets ‚Üí allows Kubernetes to use the secret when pulling from a private Docker registry.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                 5). service object

                                                                                                    A) cluster ip


*create a deployment object with docker secrets and service ( cluster ip ) and port number with 2 replicas

touch deployment.ymal : create one file
vim deployment.yaml : copy and paste


apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  labels:
    app: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-deployment
  template:
    metadata:
      labels:
        app: my-deployment
    spec:
      containers:
      - name: my-deployment
        image: vinaykumars064/cicd-repo:latest   # replace with your private image
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: my-docker-secret

---
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: my-service
spec:
  type: ClusterIP
  selector:
    app: my-service
  ports:
  - port: 8080        # Service port (internal)
    targetPort: 8080  # Container port


* cat deployment.yaml : describe


Apply YAML
* kubectl apply -f deployment.yaml

Verify Service
* kubectl get svc my-app-service


You should see something like:

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
my-app-service   ClusterIP   10.96.123.45   <none>        8080/TCP   10s


This means your pods are internally reachable within the cluster at http://my-app-service:8080


this deployment and service internally working or not
using this command to login the conatainer
* kubectl exec -it <full conatainer name > -- /bin/bash




then check :

How to correctly test?

‚úÖ From inside the cluster (a Pod):

kubectl exec -it <your-pod-name> -- curl http://172.20.85.52:8080


‚úÖ From a Service (ClusterIP):
If you already created a ClusterIP service for your deployment:

* kubectl get svc


Example output:

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
my-app-service  ClusterIP   10.100.200.15   <none>        80/TCP    10m


You can test it inside a pod:

kubectl exec -it <your-pod-name> -- curl http://my-app-service:80


***another way to create cluster ip service for existing deployment.
--------------------------------------------------------------------

* kubectl expose deployment my-deployments --type=ClusterIP --name=my-service --port=80 --target-port=80


What happens:

Type: ClusterIP ‚Üí Default service type. It makes the Service accessible only inside the cluster using the ClusterIP.

--name=my-service ‚Üí Service will be named my-service.

--port=80 ‚Üí Port the Service exposes inside the cluster.

--target-port=80 ‚Üí The port in your Pods that the Service forwards traffic to.

Check service:
*kubectl get svc my-service


You‚Äôll see something like:

NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-service   ClusterIP   10.96.172.34   <none>        80/TCP    5s

Access:

Accessible only from inside the cluster (e.g., from another Pod, or using kubectl port-forward).

Example:

curl http://10.96.172.34:80




if you not install the curl you will install the curl
Install curl inside your running pod

If your container has a package manager (apt, apk, yum), you can install curl inside:

apt update && apt install -y curl

Then:

curl http://172.20.85.52:8080

                                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------




                                                                                                 5). service object

                                                                                                    B) node port ip


Creates a Deployment object with usecases

*Uses a Secret for pulling private Docker images
*adding the 2 replicas
*Exposes the app on port 8010 (your Gunicorn app)
*Exposes the app externally using a NodePort Service
*adding the node selector


*touch deployment.yaml
*vim deployment.yaml
*cat deployment.yaml


# ---------------- Deployment ----------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment                       # Name of the deployment
  labels:
    app: my-deployment                       # Label for identifying this deployment
spec:
  replicas: 2                                # Number of pod replicas
  selector:
    matchLabels:
      app: my-deployment                     # Match pods with this label
  template:
    metadata:
      labels:
        app: my-deployment                   # Pods created will have this label
    spec:
      nodeSelector:                          # Ensures pods only schedule on nodes with this label
        vandana: vinay                       # Node must have label vandana=vinay
      containers:
      - name: my-deployment
        image: vinaykumars064/cicd-repo:latest   # Container image (update if needed)
        ports:
        - containerPort: 8010                # Gunicorn listens inside the pod on port 8010
      imagePullSecrets:
      - name: my-docker-secret               # Secret for pulling private images

---
# ---------------- Service ----------------
apiVersion: v1
kind: Service
metadata:
  name: my-service                           # Service name
  labels:
    app: my-service
spec:
  type: NodePort                             # Expose service externally via node‚Äôs IP + NodePort
  selector:
    app: my-deployment                       # Must match pod label, so traffic routes correctly
  ports:
  - port: 80                                 # Service port (clients use this)
    targetPort: 8010                         # Maps to containerPort 8010 in the pod
    nodePort: 30080                          # Fixed NodePort (between 30000‚Äì32767)





‚úÖ Apply this YAML
kubectl apply -f deployment.yaml


‚úÖ Get NodePort and Test
Get service details:

* kubectl get svc <my-app-service>

Example output:

NAME             TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
my-app-service   NodePort   10.100.200.15  <none>        80:30080/TCP   1m

* kubectl get nodes -o wide

Get your EC2 node public IP:

* curl ifconfig.me

Test from your EC2 or browser:

* curl http://<EC2-Public-IP>:30080




***another way to create nodeport ip service for existing deployment.
----------------------------------------------------------------------

** kubectl expose deployment my-deployments --type=NodePort --name=my-service --port=80 --target-port=80


Explanation of flags:

deployment my-deployments ‚Üí Refers to the Deployment you already created.

--type=NodePort ‚Üí Exposes the Service on a port of each Node (between 30000‚Äì32767).

--name=my-service ‚Üí The Service will be named my-service.

--port=80 ‚Üí The port your Service exposes (clients will use this).

--target-port=80 ‚Üí The port inside your Pod containers that traffic gets forwarded to.


Example:

If you run the above, you‚Äôll get a Service. You can check it with:

*kubectl get svc my-service


It will show something like:

NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
my-service   NodePort   10.0.23.45     <none>        80:31543/TCP   5s


Here:

80 ‚Üí Service port

31543 ‚Üí NodePort assigned (random unless you set it with --node-port=31543)

You can then access your app from outside using:

*http://<NodePublicIP>:31543





<<NODE SELECTOR>>
-------------------------

ONE WAY : when we created the node group that time also add the lables
SECOND WAY : when we created the node group that time whithout adding the lables using below mentioned steaps

üîπ Step 1: Label your EKS Nodes

Pick a node and add a label (example: vandana=vinay):

*kubectl get nodes


Output example:

NAME                                          STATUS   ROLES    AGE   VERSION
ip-10-0-3-12.ap-south-1.compute.internal      Ready    <none>   1h    v1.33.4-eks
ip-10-0-3-210.ap-south-1.compute.internal     Ready    <none>   1h    v1.33.4-eks


Now label them:

*kubectl label node ip-10-0-3-12.ap-south-1.compute.internal vandana=vinay
*kubectl label node ip-10-0-3-210.ap-south-1.compute.internal vandana=vinay

üîπ Step 2: Verify Node Labels

*kubectl get nodes --show-labels | grep vandana


You should see something like:

...,kubernetes.io/os=linux,vinay=vandana

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                   5). service object

                                                                                                    B) load balancing ip


when we create the subnets that time tag the subnets:
-----------------------------------------------------
‚úÖ Fix

You need to tag your subnets properly in AWS:

**Public Subnets (for external LoadBalancer):
Tag key: kubernetes.io/role/elb
Value: 1

**Private Subnets (for internal LoadBalancer):
Tag key: kubernetes.io/role/internal-elb
Value: 1

**All subnets used by the cluster also need:
Tag key: kubernetes.io/cluster/<your-cluster-name>
Value: owned (if created by EKS) or shared


another way to tag the subnets: You can add via AWS CLI too:
------------------------------------------------------------

aws ec2 create-tags \
  --resources <subnet-id> \
  --tags Key=kubernetes.io/role/internal-elb,Value=1 \
         Key=kubernetes.io/cluster/<your-cluster-name>,Value=shared



then

*touch deployment.yaml
*vim deployment.yaml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: test123
  labels:
    app: test123
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test123
  template:
    metadata:
      labels:
        app: test123
    spec:
      containers:
        - name: test123
          image: vinaykumars064/cicd-repo:latest
          ports:
            - containerPort: 80
      imagePullSecrets:
        - name: my-secret


*kubectl apply -f deployment.yaml
now app deployment is complited.


then creating the service deployment: in this method service is not dependent for any deployment its saparate its indipendent.
-------------------------------------------------------------------------------------------------------------------------------
touch service.yaml
vim service.yaml


apiVersion: v1
kind: Service
metadata:
  name: test123-svc
  namespace: amma
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
spec:
  selector:
    app: test123
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
  type: LoadBalancer



*kubectl apply -f service.yaml
*kubectl get svc
show external ip



***another way to create load balancing ip service for existing deployment.
-----------------------------------------------------------------------------------

*kubectl expose deployment my-deployments --type=LoadBalancer --name=my-service --port=80 --target-port=80


What happens:

Type: LoadBalancer ‚Üí Creates an external cloud load balancer (like AWS ELB, GCP LB, Azure LB) and assigns an external IP or hostname.

--port=80 ‚Üí Clients use this port to connect.

--target-port=80 ‚Üí Traffic gets forwarded to port 80 in the Pod containers.

--name=my-service ‚Üí Service is named my-service.

Check service:
*kubectl get svc my-service


You‚Äôll see something like:

NAME         TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)        AGE
my-service   LoadBalancer   10.96.33.120   a1b2c3d4.elb.amazonaws.com   80:31234/TCP   5s


Cluster-IP: Internal service IP

EXTERNAL-IP: Public IP or DNS of your load balancer (may take a few minutes to appear).

80:31234 ‚Üí 80 is service port, 31234 is nodeport backing the LB.

Access:

Once the EXTERNAL-IP is ready, you can test it:

curl http://<EXTERNAL-IP>:80


‚ö†Ô∏è Since you‚Äôre using EKS (AWS) from your earlier messages:

Make sure your worker nodes have IAM policies that allow creating ELBs.

Subnets must be public (with internet gateway) if you need a public external IP.


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                   HORIZONTAL POD AUTOSCALING (HPA)


touch deployment.yaml
vim deployment.yaml


‚úÖ 1. Update Deployment with nodeSelector

Here‚Äôs your Deployment YAML with nodeSelector added (key = vinay, value = vandana):


apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
  labels:
    app: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      nodeSelector:
        vinay: vandana                  # Pods will only schedule on nodes with this label
      containers:
      - name: my-app-container
        image: your-docker-user/your-private-image:latest
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: my-docker-secret



‚úÖ 2. Create HPA using command

You can create an HPA (autoscaler) for this Deployment using kubectl autoscale:

*kubectl autoscale deployment my-app-deployment \
  --cpu-percent=50 \
  --min=2 \
  --max=5


--cpu-percent=50 ‚Üí Target average CPU utilization across Pods is 50%.

--min=2 ‚Üí Minimum 2 Pods.

--max=5 ‚Üí Maximum 5 Pods.

Check HPA status:

*kubectl get hpa


another way
--------------
‚úÖ 3. HPA YAML file

If you prefer YAML, here‚Äôs the equivalent:


apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app-deployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50


Apply it:

-kubectl apply -f my-app-hpa.yaml


‚ö†Ô∏è Important: For HPA based on CPU to work, you need the Metrics Server installed in your cluster (EKS doesn‚Äôt install it by default).


how to check
--------------

üîç 1. List all HPAs
*kubectl get hpa


Example output:

NAME          REFERENCE                   TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
my-app-hpa    Deployment/my-app-deployment   60%/50%   2         5         3         2m


TARGETS ‚Üí 60%/50% means current CPU usage is 60%, target is 50%.

REPLICAS ‚Üí Current number of pods.

MINPODS / MAXPODS ‚Üí Range you set in HPA.

üîç 2. Describe details of HPA
*kubectl describe hpa my-app-hpa


You‚Äôll see:

Which deployment it‚Äôs scaling (Scale target).

Current vs. desired replicas.

CPU utilization metrics.

Events (if scaling happened).

üîç 3. Check deployment replicas after scaling
*kubectl get deployment my-app-deployment


This shows how many pods are running and desired:

NAME                READY   UP-TO-DATE   AVAILABLE   AGE
my-app-deployment   4/4     4            4           10m

üîç 4. Watch pods while scaling

You can keep watching pods scaling up/down:

*kubectl get pods -l app=my-app -w


‚ö†Ô∏è Reminder: If you don‚Äôt see CPU values in HPA (<unknown> instead of %), then Metrics Server is not installed.



check the metric server installed or not
------------------------------------------

*kubectl get pods -n kube-system | grep metrics-server

You should see a pod like:
metrics-server-6c9c8c9b5c-abc12   1/1   Running   0   5m

if in case not installed now install using these steps:
------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                       Taints and Tolerations:
                                                                                     ----------------------------

step by step with Kubernetes Taints and Tolerations:

1. What are Taints and Tolerations?

Taint (applied on Node): Stops pods from scheduling on a node unless the pod has a matching toleration.

Toleration (applied on Pod): Allows a pod to ‚Äútolerate‚Äù the taint and get scheduled on that node.

üëâ Think of it like:

Taint = Keep out sign on the node

Toleration = Special pass for pods to enter

2. Apply a Taint on a Node

Command:

*kubectl taint nodes <node-name> key=value:NoSchedule


Example:

*kubectl taint nodes worker-node1 role=database:NoSchedule


This means: only pods with a toleration for role=database can run on worker-node1.

NoSchedule ‚Üí Pods without toleration won‚Äôt be scheduled here.

Other effects:

NoSchedule ‚Üí don‚Äôt schedule pods unless toleration exists.

PreferNoSchedule ‚Üí try to avoid scheduling but not strict.

NoExecute ‚Üí evict existing pods if they don‚Äôt have toleration.

3. Add Toleration in Pod Spec

You add tolerations in the pod YAML under spec.tolerations.

Example pod tolerating the above taint:

apiVersion: v1
kind: Pod
metadata:
  name: my-db-pod
spec:
  containers:
  - name: my-db-container
    image: mysql:5.7
  tolerations:
  - key: "role"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"

4. Verify Taints on Node
kubectl describe node <node-name> | grep Taints

5. Remove a Taint
kubectl taint nodes <node-name> key=value:NoSchedule-


(Note the - at the end ‚Üí removes the taint)

‚úÖ Summary:

Taint nodes to restrict which pods can run there.

Tolerations in pods allow them to bypass the restriction.


my example


 mainly two ways to taint a node
---------------------------------

*one way is go to node group select node group edit node group add "Kubernetes taints" then next.
*another way is used to command :  kubectl taint nodes <node-name> appa=amma:NoSchedule

and then add the toleration in deployment.yaml file example

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
  labels:
    app: my-app
spec:
  replicas: 2                                # number of Pods to run
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: vinaykumars064/cicd-repo:latest   # replace with your private image
        ports:
        - containerPort: 8080                               # expose port inside container
      imagePullSecrets:
      - name: my-docker-secret
      tolerations:
      - key: "appa"
        operator: "Equal"
        value: "amma"
        effect: "NoSchedule"


* kubectl apply -f deployment.yaml
* kubectl get pods

Verify Taints on Node:
* kubectl describe node <node-name> | grep Taints

Remove a Taint:
* kubectl taint nodes <node-name> key=value:NoSchedule-

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
